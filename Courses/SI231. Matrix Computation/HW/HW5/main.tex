
\documentclass[english,onecolumn]{IEEEtran}
\usepackage[T1]{fontenc}
\usepackage[latin9]{luainputenc}
\usepackage[letterpaper]{geometry}
\geometry{verbose}
\usepackage{amsfonts}
\usepackage{babel}
\usepackage{ulem}

\usepackage{extarrows}
\usepackage[colorlinks]{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[ruled,linesnumbered]{algorithm2e}

\usepackage{amsmath,graphicx}
\usepackage{subfigure} 
\usepackage{cite}
\usepackage{amsthm,amssymb,amsfonts}
\usepackage{textcomp}
\usepackage{bm,pifont}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xparse}
\usepackage{cleveref}
\usepackage{xcolor}
\usepackage[center]{caption}

\definecolor{salmon}{rgb}{1, 0.5020, 0.4471}

\NewDocumentCommand{\codeword}{v}{
\texttt{\textcolor{blue}{#1}}
}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
\topmargin            -18.0mm
\textheight           226.0mm
\oddsidemargin      -4.0mm
\textwidth            166.0mm
\def\baselinestretch{1.5}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\Pb}{\mathbf{P}}
\newcommand{\Ib}{\mathbf{I}}
\newcommand{\vb}{\mathbf{v}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Wcal}{\mathcal{W}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\bigS}{\mathcal{S}}
\newcommand{\bA}{{\bf A}}
% \newcommand{\bU}{{\bf U}}
\newcommand{\bV}{{\bf V}}
\newcommand{\bY}{{\bf Y}}
\newcommand{\bX}{{\bf X}}
\newcommand{\bmu}{{\boldsymbol \mu}}
\newcommand{\bSig}{{\boldsymbol \Sigma}}
\newcommand{\bone}{{\boldsymbol 1}}
\newcommand{\bQ}{{\bf Q}}
\newcommand{\bR}{{\bf R}}
\newcommand{\bH}{{\bf H}}
\newcommand{\bU}{{\bf U}}
\newcommand{\bT}{{\bf T}}
\newcommand{\bI}{{\bf I}}
\newcommand{\bq}{{\bf q}}
\newcommand{\bz}{{\bf z}}
\newcommand{\bL}{{\bf L}}
\newcommand{\bx}{{\bf x}}
\newcommand{\bv}{{\bf v}}
\newcommand{\bG}{{\bf G}}
\def\A{\mathbf{A}}
\def\v{\mathbf{v}}

\begin{document}

\begin{center}
	\textbf{\LARGE{SI231 - Matrix Computations, Fall 2020-21}}\\
	{\Large Homework Set \#5}\\
	\texttt{Prof. Yue Qiu and Prof. Ziping Zhao}\\
	\texttt{\textbf{Name:}}   	\texttt{Tao Huang}  		\hspace{1bp}
	\texttt{\textbf{Major:}}  	\texttt{ Undergraduate in CS } 	\\
	\texttt{\textbf{Student No.:}} 	\texttt{ 2018533172}     \hspace{1bp}
	\texttt{\textbf{E-mail:}} 	\texttt{ huangtao1@shanghaitech.edu.cn}
\par\end{center}

\noindent
\rule{\linewidth}{0.4pt}
{\bf {\large Acknowledgements:}}
\begin{enumerate}
    \item Deadline: \textcolor{red}{\textbf{2020-12-07 23:59:00}}
    \item Submit your homework at \textbf{Gradescope}.
    Homework \#5 contains two parts, the theoretical part the and the programming part.
    \item About the the theoretical part:
    \begin{enumerate}
            \item[(a)] Submit your homework in \textbf{Homework 5} in gradescope. Make sure that you have correctly select pages for each problem. If not, you probably will get 0 point.
            \item[(b)] Your homework should be uploaded in the \textbf{PDF} format, and the naming format of the file is not specified.
            \item[(c)] No handwritten homework is accepted. You need to use \LaTeX $\,$ in principle.
            \item[(d)] Use the given template and give your solution in English. Solution in Chinese is not allowed. 
        \end{enumerate}
  \item About the programming part:
  \begin{enumerate}
      \item[(a)] Submit your codes in \textbf{Homework 5 Programming part} in gradescope. 
      \item[(b)] When handing in your homework in gradescope, package all your codes into {\sf your\_student\_id+hw4\_code.zip} and upload. In the package, you also need to include a file named {\sf README.txt/md} to clearly identify the function of each file. (\textcolor{blue}{".zip" format package rather than ".rar, .7zip" should be uploaded, solution of the results should be named according to requirements}.)
     \item[(c)] Make sure that your codes can run and are consistent with your solutions.
  \end{enumerate}
  \item \textbf{Late Policy details can be found in the bulletin board of Blackboard.}
\end{enumerate}
\rule{\linewidth}{0.4pt}

\section*{Study Guide}
\noindent
This homework concerns the following topics:
\begin{itemize}
    \item \textbf{Regularization}, see Lecture 8 of Zhao, Lecture 20 of Qiu.
    \item \textbf{SVD computation}, see Lecture 7 of Zhao, Lecture 18 of Qiu. 
    \item \textbf{Iterative methods}, see Lecture 2 of Zhao, Lecture 21 of Qiu. 
    \item \textbf{Application of SVD, PCA}, see Lecture 7 of Zhao, Lecture 19 of Qiu. 
\end{itemize}

\newpage
\section{Least square with regularization}
\noindent\textbf{Problem 1}. 
In this problem, we will learn how to solve LS when there is noise. for some $\lambda>0$, where the term $\lambda||\mathbf{x}||^{k}_{k}$ is added to improve the system conditioning, thereby attempting to reduce noise sensitivity. Usually, we set $k=1$ which is called $\ell$1 regularization, or set $k=2$ which is called $\ell$2 regularization. 

\noindent $\ell$1 regularized LS:
\begin{equation}
    \mathbf{x_{LS}}=\min_{\mathbf{X}\in\mathbf{\Rbb}^{n}}f(\mathbf{x}), \quad f(\mathbf{x})=||\mathbf{Ax-y}||^{2}_{2}+\lambda||\mathbf{x}||_{1}
\end{equation}
\noindent $\ell$2 regularized LS:
\begin{equation}
    \mathbf{x_{LS}}=\min_{\mathbf{X}\in\mathbf{\Rbb}^{n}}f(\mathbf{x}),\quad f(\mathbf{x})=||\mathbf{Ax-y}||^{2}_{2}+\lambda||\mathbf{x}||^{2}_{2}
\end{equation}

Write programs to solve the regularized least square problem, any programming language is suitable. where $\mathbf{A} \in \Rbb^{m \times n}$ is a matrix representing the predefined data set with $m$ data samples of $n$ dimensions ($m$=1000, $n$=210), and $\mathbf{y} \in \Rbb^m$ represents the labels. The data samples are provided in the "data.txt" file, and the labels are provided in the "label.txt" file, you are supposed to load the data before solving the problem. In this problem, we set $\lambda=0.1$ for two algorithms.
\begin{enumerate}
    \item (\textcolor{blue}{\textbf{8 points}}) Solve $\ell$1 regularized LS using Majorization-Minimization method, we set $c=1e+5$ in this problem. The Majorization-Minimization method for solving problem updates $ {\bf x}$ as
    $$
        {\bf x^{(k+1)}} = \mathbf{soft}(\frac{1}{c}\mathbf{A^{T}}(\mathbf{y}-\mathbf{Ax^{(k)}})+\mathbf{x^{(k)}},\lambda/c),
    $$
 where $\mathbf{soft}$ is called the soft-thresholding operator and is defined as follows: if $\mathbf{z=soft(x,\sigma)}$, then $z_{i}=sign(x_{i})\max\{|x_{i}|-\sigma,0\}$.
    \item (\textcolor{blue}{\textbf{8 points}}) Solve $\ell$2 regularized LS using gradient descent method. The gradient descent method for solving problem updates $ {\bf x}$ as
    $$
        {\bf x} = {\bf x} - \gamma \cdot \nabla_{{\bf x}} f(\mathbf{x}),
    $$
 where $\gamma$ is the step size of the gradient decent methods. We suggest that you can set $\gamma=1e-5$. 
    \item (\textcolor{blue}{\textbf{4 points}}) Compare two methods above. 
    \begin{enumerate}
        \item[(a)] compare L0 norm (the number of non-zero elements) of $\mathbf{x_{LS}}$ computed by above two algorithms;
        \item[(b)] Compare the loss $||\mathbf{Ax-y}||^{2}_{2}$ for results $\mathbf{x}=\mathbf{x_{LS}}$ of above two algorithms.
    \end{enumerate}
\end{enumerate}

\noindent\textbf{Remarks: }
\begin{itemize}
    \item The solution of the two methods should be printed in files named "sol1.txt" and "sol2.txt" and submitted in gradescope.  The format should be same as the input file (210 rows plain text, each rows is a dimension of the final solution).
    \item Make sure that your codes are executable and are consistent with your solutions.
\end{itemize}

\noindent
\textbf{Solution.}
\begin{itemize}
	\item $L0$ norm:
	\begin{itemize}
		\item MM: 210.
		\item GD: 210.
	\end{itemize}
	Two methods share same number of non-zero elements.
	\item Loss:
	\begin{itemize}
		\item MM: 2946.638577599585.
		\item GD: 2945.528238789967.
	\end{itemize}
	Gradient descent has slightly lower loss than Majorization-Minimization method.
\end{itemize}

\newpage
\section{Computations of SVD}
\noindent\textbf{Problem 2}.
In this problem you will see singular value stability and discover computation of SVD.
\begin{enumerate}
    \item (\textcolor{blue}{\textbf{4 points}}) Consider a $4 \times 4$ matrix
    \[
	\mathbf{A} = \begin{bmatrix}
		0& 1&0&0 \\
		0&0&2&0\\
		0&0&0&3\\
		\frac{1}{6000}&0&0&0
	\end{bmatrix}\,,
	\]
	find its singular values and eigenvalues. In this sub-problem you can use eigenvalue decomposition ('eig') and singular value decomposition function ('svd') in Matlab or Python.
	\item (\textcolor{blue}{\textbf{8 points}}) Compute the SVD decomposition of a $3 \times 2$ matrix by \Cref{alg:svd}
	\[
	\mathbf{B} = \begin{bmatrix}
		3& 2 \\
		1&4  \\
		\frac{1}{2}&\frac{1}{2}
	\end{bmatrix}\,.
	\]
Are the results right? If not, can you get the correct answer through the results of \Cref{alg:svd}? The In this sub-problem you can use the eigenvalue decomposition ('eig') and QR decomposition function ('qr') in Matlab or Python.
\item (\textcolor{blue}{\textbf{8 points}}) Consider an $m\times m$ upper triangular matrix with 0.1 on the main diagonal and 1 everywhere above the diagonal. Fine the smallest singular value through the singular value decomposition function ('svd') and \cref{alg:svd}. You are required to plot two curves on a log scale for $m=1,\dots,30$ and show which one is the desired result.
\end{enumerate}

\begin{algorithm}[htbp]
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\caption{SVD Decomposition by $\mathbf{A}^T\mathbf{A}$}
\label{alg:svd}
\SetAlgoLined
\Input{Thin matrix A}
Form $\mathbf{A}^T\mathbf{A}$.\\
Compute the eigenvalue decomposition $\mathbf{A}^T\mathbf{A}$=$\mathbf{V}^T\mathbf{\Lambda V}$.\\
Let $\mathbf{\Sigma}$ be an m$\times$n diagonal matrix with diagonal entries being the nonnegative square root of diagonal entries of $\mathbf{\Lambda}$.\\
Solve the system $\mathbf{U\Sigma} = \mathbf{AV} $for orthogonal matrix $\mathbf{U}$ (e.g., via QR factorization). \\
\Output{$\mathbf{U}, \mathbf{\Sigma}, \mathbf{V}$.}

\end{algorithm}
{\bf Remarks}
\begin{enumerate}
    \item Singular values cannot be negative.
    \item Please show your result and insert figures in your PDF.
\end{enumerate}

\noindent
\textbf{Solution.}
\begin{enumerate}
	\item 
		\begin{itemize}
			\item Singular Values: 3.000, 2.000, 1.000, 1.666$e-04$.
			\item Eigenvalues: $-0.177827941003892 + 0.000000000000000i, \quad 0.000000000000000 + 0.177827941003892i,   \\\quad0.000000000000000 - 0.177827941003892i, \quad 0.177827941003892 + 0.000000000000000i$.
		\end{itemize}
	\item The result is not right, which is shown as below:
	\begin{align*}
		U &= \begin{pmatrix}
-0.636439762626821&	0.756963149060323	 &-0.148159439497438\\
-0.759694244715964	&-0.648402367481179	 &-0.0493864798324795\\
-0.133450676629821	&0.0811243539849895	&0.987729596649590
		\end{pmatrix}\\
		\Sigma & = \begin{pmatrix}
-5.16279911752889	&0\\
0	&-1.96099598980797\\
0	&0
		\end{pmatrix}\\
		V &= \begin{pmatrix}
0.529894502697747&	-0.848063568378401\\
0.848063568378401&	0.529894502697747
		\end{pmatrix}
	\end{align*}
	The reason comes from that qr method is not unique and it doesn't ensure the positivity of singular values. So we need to tune the positivity of $\Sigma$. In the followings, we give the modified verison of the results:
		\begin{align*}
		U &= \begin{pmatrix}
0.636439762626821	&-0.756963149060323	&-0.148159439497438\\
0.759694244715964	&0.648402367481179	&-0.0493864798324795\\
0.133450676629821	&-0.0811243539849895	&0.987729596649590
\end{pmatrix}\\
		\Sigma & = \begin{pmatrix}
5.16279911752889	&0\\
0	&1.96099598980797\\
0	&0
\end{pmatrix}\\
		V &= \begin{pmatrix}
0.529894502697747&	-0.848063568378401\\
0.848063568378401&	0.529894502697747
\end{pmatrix}
	\end{align*}
	\item The result from 'svd' is the desired the one since it is more numerical stable than algorithm \ref{alg:svd}.
		 \begin{figure}[htbp]
		\centering
		\includegraphics[scale=0.4]{figures/2_3.png}
		\caption{Log of smallest singular value.}
	\end{figure}
\end{enumerate}

\clearpage
\newpage
\section{Iterative methods.}
\noindent\textbf{Problem 3}.
In this problem, you will learn how to implement there iterative methods, Jacobi, Gauss-Seidel and SOR Methods to solve the linear equation $\mathbf{b}=\mathbf{Ax}$, and compare the convergence of these methods. 
"A1.txt", "A2.txt","b1.txt", "b2.txt","x1.txt", "x2.txt" is the data of the $\mathbf{b}, \mathbf{x}, \mathbf{A}$ with different size (10 and 1000 respectively).
\begin{algorithm}
	\caption{Iterations method}
	\label{algo:Jacobi}
    \LinesNumbered
	\KwIn{A starting point $\mathbf{x}^{0}$, maximum iterations $N$, error bound $\mathrm{eps}$}
    \KwOut{Solution $\mathbf{x}$ of $\mathbf{Ax}=\mathbf{b}$}
	\For{$k=0,1,2,\dots,N$}{
	    \textbf{update} $\mathbf{x}^k$ to obtain $\mathbf{x}^{k+1}$ \tcp*{Use one of the three iteration methods}
        \If{$\|\mathbf{x}^{k+1}-\mathbf{x}^{k}\|_2< \mathrm{eps}$ }{
            \textbf{break}\;
        }
    }
\end{algorithm}

Suppose $\mathbf{x}\in\mathbb{R}^{n}$, $\mathbf{A}=[a_{ij}]\in\mathbb{R}^{n\times n}$ and $\mathbf{b}=(b_i)\in\mathbb{R}^{n}$.

Each iteration method uses different updates:

For Jacobi Iteration, 
\begin{equation*}
    x_i^{k+1} = (b_i - \sum_{i \neq j} a_{ij}x_j^k) / a_{ii} \quad \text{for }\ i = 1,\dots, n,
\end{equation*}

For Gauss-Seidel Iteration, 
\begin{equation*}
    x_i^{k+1} = (b_i - \sum_{j > i} a_{ij}x_j^{k+1} - \sum_{j < i} a_{ij}x_j^{k}) / a_{ii}, \quad \text{for }\ i = 1,\dots, n,
\end{equation*}

For SOR Iteration with relaxation factor $\omega$, 
\begin{equation*}
    x_i^{k+1} = (1-\omega) x_i^{k} + \omega \left(b_i - \sum_{j>i} a_{ij}x_j^{k+1} - \sum_{j<i} a_{ij}x_j^{k}\right) / a_{ii}, \quad \text{for }\ i = 1,\dots, n.
\end{equation*}

\textbf{Remarks:}
\begin{enumerate}
    \item (\textcolor{blue}{\textbf{8 points}}) Implement the Jacobi Iteration and Gauss-Seidel Iteration methods.
    \item (\textcolor{blue}{\textbf{8 points}}) Implement the SOR Iteration method, tune the relaxation factor $\omega$ to achieve high convergence rate and plot the curve of number of iterations v.s. the value of $\omega$.
    \item (\textcolor{blue}{\textbf{4 points}}) Plot the error $\|\mathbf{x}_{\mathrm{true}} - \mathbf{x}^k\|_2$ for $k=1,2,\dots,N$, analyze the convergence rate and the final error of three methods.
\end{enumerate}
Note: The value of the ``eps'' in Terminal condition should not be too large!



\noindent
\textbf{Solution.}
\begin{enumerate}
	\item See codes.\pagebreak
	\item We choose $epsi = 1e-4$.
		\begin{figure}[htbp]
		\begin{minipage}[t]{0.45\linewidth}
		\centering
\includegraphics[scale=0.4]{figures/3_iteration_10.png}
\caption{Size = 10.}
		\end{minipage}%
		\begin{minipage}[t]{0.45\linewidth}
	\centering
\includegraphics[scale=0.4]{figures/3_iteration_1000.png}
\caption{Size = 1000.}
		\end{minipage}
	\end{figure}

	\item We find that Jacobi converges most fastly and SOR converges most slowly. The share the nearly same final error. 
			\begin{figure}[htbp]
		\begin{minipage}[t]{0.45\linewidth}
		\centering
\includegraphics[scale=0.4]{figures/3_error_10.png}
\caption{Size = 10.}
		\end{minipage}%
		\begin{minipage}[t]{0.45\linewidth}
	\centering
\includegraphics[scale=0.4]{figures/3_error_1000.png}
\caption{Size = 1000.}
		\end{minipage}
	\end{figure}

\end{enumerate}


\newpage
\section{Application of SVD}
\noindent\textbf{Problem 4}.
In this problem you will see one of the application of SVD, what is doing face recognition on Yale B dataset. Here we will only use $4$ individuals under $15$ relatively brighter different illumination conditions.

\begin{figure}[h]
    \centering
    \includegraphics{figures/example_face.png}
    \caption{Face images of one individual under 5 different illumination conditions in the extended YaleB dataset. All images are frontal faces.}
    \label{fig:my_label}
\end{figure}
\vspace{-0.3cm}
Face recongition is an area of computer vision in which low-dimensional linear models such as principal component analysis (PCA) and its variations have been popular tools for capturing the variability of face images. And it has been shown that PCA can be solved by SVD optimally. In the following questions you will see how \Cref{Alg:PCA} can be used.

\vspace{0.5cm}
\begin{algorithm}[H]
\textbf{Parameters}:\\
$\bX$: \quad $D\times N $ data matrix.\\
$d$: \quad Number of principal components.\\
\textbf{Returned values}:\\
$\bmu$: \quad Mean of the data.\\
$\bU$: \quad Orthonormal basis for the subspace.\\
$\bY$: \quad Low-dimensional representation( or principal components).\\
\textbf{Description}:\\
Compute the SVD of the data matrix $\bX - \bmu \bone^T = \bU \bSig \bV^T$.\\
Sort the singular values in descending order, choose the first $d$ singular values and the corresponding $d$ left singular vectors $\bU_d$.\\
Find the $d$ principal components $\bY = \bU_d^T(\bX - \bmu \bone^T)$
\caption{[$\bmu, \bU, \bY$] = PCA\_via\_SVD($\bX,d$) }
\label{Alg:PCA}
\end{algorithm}
\vspace{0.5cm}

The principal bases $\bU$ estimated by PCA are also known as the \textit{eigenfaces} in the computer vision literature. The first eigenface is the left singular vector corresponding to the largest singular value, the second eigenface is the left singular vector corresponding to the second largest singular value and so on.

\begin{enumerate}
    \item (\textcolor{blue}{\textbf{4 points}}) Apply PCA in \Cref{Alg:PCA} with $d = 10$ to individual 4\footnote{you can use the images or the mat file in data1 folder}. Plot the mean face $\bmu$, the first eigenface, the second eigenface, the third eigenface and the tenth eigenface. Describe what you have observed.
    \item (\textcolor{blue}{\textbf{8 points}}) Apply PCA in \Cref{Alg:PCA} with $d = 10$ to the Training Set. 
    \begin{enumerate}
        \item Plot the mean face, the first eigenface, the second eigenface, the third eigenface and the tenth eigenface.
        \item Plot the sorted singular values.
        \item Project the Test Set onto the face subspace given by PCA, that is $\bY_{test} = \bU_d^T(\bX_{test}-\bmu \bone^T)$. Compute the projected faces, that is $\text{Proj}(\bX_{test}) = \bmu \bone^T + \bU_d\bY_{test}$. Then choose one projected face for each individual and plot them. And show those projected faces for $d = 2$ again.
    \end{enumerate} 
    \item (\textcolor{blue}{\textbf{8 points}}) Classify these test faces using 1-nearest-neighbor, that is, label an image $x$ as corresponding to individual $i$ if its projected image $y$ is closest to one of projected training image $y_j$ of individual $i$\footnote{we use $\ell_2$ norm as the distance measurement: $ \|y-y_j\|_2$}. Report the percentage of incorrectly classified face images for $d = 2,...,8$ and put them into the following table. Which value of $d$ gives the best recognition performance?
    \begin{table}[h]
        \centering
        \caption{Summary of errors}
        \begin{tabular}{c|c|c|c|c|c|c|c}
        \# eigenvectors& 2 & 3& 4 & 5 & 6 & 7 & 8 \\ 
        \hline
            error & 0.55 & 0.35 & 0.30 & 0.25 & 0.10 & 0 & 0  
        \end{tabular}
        \label{tab:my_label}
    \end{table}
\end{enumerate}
{\bf Remarks:}
\begin{enumerate}
    \item Data are provided in two forms: images and mat data. You only need to use one of them. 
    \item $10$ images for each individuals in the training data, and $5$ images for each individuals in the test data. All images are of the same size $48\times 42$.
    \item In mat file \textbf{all\_data.mat}, \textbf{data\_train} and \textbf{data\_test} are the matrices of training data and test data where each column is the column stack of an image\footnote{the column stack of an image matrix $\bX\in \Rbb^{m\times n}$ is denoted by $x = [X_{11},X_{21},X_{31},...,X_{mn}]^T\in\Rbb^{mn}$}, \textbf{Y\_label\_train} and \textbf{Y\_label\_test} are ground truth labels.
    \item `Plot the mean face or eigenface' means you should reshape the vector to the size of image and show it.
    \item Recommend to use truncated(thin) SVD for fast implementation.
    \item Please \textbf{insert your figures in your PDF}.
\end{enumerate}

\noindent
\textbf{Solution.}
\begin{enumerate}
	\item We find that the eigen faces corresponds to the larger singular value have better performance. \begin{figure}[htbp]
		\centering
		\includegraphics[scale=0.25]{figures/4_a_1.png}
		\caption{Mean face of individual 4.}
	\end{figure}
 \begin{figure}[htbp]
	\centering
	\vspace{-1cm}
	\includegraphics[scale=0.5]{figures/4_a_2.png}
	\vspace{-3cm}
	\caption{\{1,2,3,10\} Eigen faces}
\end{figure}

\pagebreak
\item 
\begin{enumerate}
	\item As follows:
	 \begin{figure}[htbp]
		\centering
		\includegraphics[scale=0.25]{figures/4_b_1_1.png}
		\caption{Mean face of the trainning set.}
	\end{figure}
	 \begin{figure}[htbp]
		\centering
		\vspace{-0.5cm}
		\includegraphics[scale=0.15]{figures/4_b_1_2.png}
		\vspace{-0.2cm}
		\caption{\{1,2,3,10\} Eigen faces}
	\end{figure}
	\item As follows:
		 \begin{figure}[htbp]
		\centering
		\includegraphics[scale=0.2]{figures/4_b_2.png}
		\caption{Sorted singular values.}
	\end{figure}
	\pagebreak
	\item As follows:
			 \begin{figure}[htbp]
		\centering
		\includegraphics[scale=0.2]{figures/4_b_3.png}
		\caption{d=10}
	\end{figure}
\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.15]{figures/4_b_3_2.png}
	\caption{d=2}
\end{figure}
\pagebreak
\end{enumerate}
\item $d=7,8$ give the best recognition performance, where error = 0. Also, we see that the performance improves when $d$ gets larger. This is intuitively right since more principal components result in higher accuracy.
    \begin{table}[h]
	\centering
	\caption{Summary of errors}
	\begin{tabular}{c|c|c|c|c|c|c|c}
		\# eigenvectors& 2 & 3& 4 & 5 & 6 & 7 & 8 \\ 
		\hline
		error & 0.55 & 0.35 & 0.30 & 0.25 & 0.10 & 0 & 0  
	\end{tabular}
	\label{tab:my_label}
\end{table}
\end{enumerate}


\clearpage
\newpage
\noindent\textbf{Problem 5}.
In this problem, we will learn how to use SVD to compress, or reduce the dimension of the data. The real data is with low rank internal, but often corrupted, or contaminated by noise, which leads to the full rank of the data matrix $\mathbf{A}\in\mathbb{R}^{D\times N}$ . To reduce the storage consumption of the real data (note that $D$ is extremely large in practical), we seek to find a low rank approximation to  $\mathbf{A}$, that is, we want to solve the problem
\begin{align}
    \min_{\mathbf{X}\in\mathbb{R}^{D\times N}}&\quad \|\mathbf{X}-\mathbf{A}\|_F^2\\
    \mathrm{s.t.}&\quad \mathrm{rank}(\mathbf{X})\leq d
\end{align}
where $\|\cdot\|_F$ is the Frobenius norm, $d<< D$ is unknown.
By the Theorem of PCA via SVD, the above problem is equivalent to the following problem (suppose the eigenvalues of $\mathbf{A}$ are given by $\sigma_1(\mathbf{A})\geq\sigma_2(\mathbf{A})\geq\cdots\geq \sigma_K(\mathbf{A})\geq0$ and $K=\min\{D,N\}$):
\begin{equation}
    \min_{d=1,\dots,K}J(d)=\sum_{i=d+1}^K\sigma_i^2(\mathbf{A})
\end{equation}
However, this is not a good criterion, since the optimal solution is given by $d^*=\mathrm{rank}(\mathbf{A})$ ($J(d^*)=0$).

The problem of determining the optimal dimension $d$ is in fact a "model selection" problem, which is due to the fact that choice of $d$ balances the complexity of the model and the storage of the data. There are many model selection criterion, we will learn two of them.

\begin{enumerate}
    \item The first way is to bound the residual of approximation, that is, suppose $\hat{\mathbf{X}}$ is the best $\mathrm{rank}-d$ approximation of $\mathbf{A}$, given a threshold $\tau>0$, we want to minimize the residual with respect to Frobinius-norm, that is
    \begin{align}
        \min_{d=1,\dots,K}&\quad \|\mathbf{A}-\hat{\mathbf{X}}\|_F^2\leq \tau
    \end{align}
    the problem can be explicitly written as
    \begin{equation}\label{eq:2_norm_min}
        d^*=\min_{d=1,\dots,K}\left\{d\left|\sum_{i=d+1}^K\sigma_{i}^2(\mathbf{A})\leq \tau\right.\right\}
    \end{equation}
    
    \item Note that the first criterion (\ref{eq:2_norm_min}) depends on specific problem (singular values of data matrix are not invariant with respect to linear transformations), it is hard to chose a reasonable threshold. To generalize our criterion, we consider the normalized version of (\ref{eq:2_norm_min}), this new criterion, \textit{a.k.a} \textbf{variance explained ratio} in machine learning, is given by
    \begin{equation}\label{eq:accumulated_error}
        d^*=\min_{d=1,\dots,K}\left\{d\left|\frac{\sum_{i=d+1}^K\sigma_{i}^2(\mathbf{A})}{\sum_{i=1}^K\sigma_i^2(\mathbf{A})}\leq \tau\right.\right\}
    \end{equation}
\end{enumerate}
Now, suppose the data matrix $\mathbf{A}\in\mathbb{R}^{D\times N}$ is the same as {\color{blue} Problem 4} (\emph{data1/data1.mat}). You are required to 
\begin{enumerate}
    \item (\textcolor{blue}{\textbf{8 points}}) Plot the squared singular values of $\mathbf{A}$  along with the threshold $\tau=150$, what is the solution to (\ref{eq:2_norm_min})?
    \item (\textcolor{blue}{\textbf{12 points}, \textbf{6 points} for plot, \textbf{6 points} for table}) Plot the figure of function 
    \begin{equation}
        f(d)=\frac{\sum_{i=d+1}^K\sigma_{i}^2(\mathbf{A})}{\sum_{i=1}^K\sigma_i^2(\mathbf{A})},\quad d=1,\dots,K
    \end{equation}
    and fill the following table from the figure with respect to (\ref{eq:accumulated_error}):
    \begin{table}[htb]
    \centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
    \multicolumn{1}{|c|}{$\tau$} & 0.1 & 0.05 & 0.02 & 0.005 \\ \hline
    Compression rate            &0.1005 &  0.2011 &  0.3016  &  0.4022   \\ \hline
    \end{tabular}
    \caption{The compression rate with respect different threshold}
    \end{table}
    
    where the compression rate is defined as 
    \begin{equation}
        \text{compression rate}=\frac{\#\{\text{entries in } \hat{\mathbf{X}}\}}{\#\{\text{entries in } \mathbf{A}\}}=\frac{d^*(D+N+1)}{DN}
    \end{equation}
\end{enumerate}



{\bf Remarks}
\begin{enumerate}
    \item Please \textbf{insert your figures in your PDF}.
    \item You can just mark the solution of problem 1) on your figure (\textbf{plot a vertical line with red color or mark the solution with marker}).
    \item Please draw a continuous curve for problem 2), see \textit{stairs} function in Matlab, \textit{plt.step} function in Python for more details.
    \item For simplicity, we omit some proof details, you may refer to \textit{Generalized Principal Component Analysis, Section 2.1} for more details.
\end{enumerate}

\noindent
\textbf{Solution.}
\begin{enumerate}
	\item We find that the solution $d^\star = 2$.
	\begin{figure}[htbp]
		\begin{minipage}[t]{0.45\linewidth}
		\centering
\includegraphics[scale=0.4]{figures/5_1_1.png}
\caption{Squared singular values.}
		\end{minipage}%
		\begin{minipage}[t]{0.45\linewidth}
	\centering
\includegraphics[scale=0.4]{figures/5_1_2.png}
\caption{Cumulative sum of squared singular values.}
		\end{minipage}
	\end{figure}
\pagebreak
\item As follows:
	\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.4]{figures/5_2_1.png}
	\caption{Figure of $f(d)$.}
\end{figure}\\
    \begin{table}[htb]
	\centering
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		\multicolumn{1}{|c|}{$\tau$} & 0.1 & 0.05 & 0.02 & 0.005 \\ \hline
		Compression rate            &0.1005 &  0.2011 &  0.3016  &  0.4022   \\ \hline
	\end{tabular}
	\caption{The compression rate with respect different threshold}
\end{table}
\end{enumerate}

\end{document}