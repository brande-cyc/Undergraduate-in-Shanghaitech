\documentclass[10pt]{article}
\usepackage[UTF8]{ctex}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage{amsthm}  
\usepackage{amsmath,amscd}
\usepackage{amssymb,array}
\usepackage{amsfonts,latexsym}
\usepackage{graphicx,subfig,wrapfig}
\usepackage{times}
\usepackage{psfrag,epsfig}
\usepackage{verbatim}
\usepackage{tabularx}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{cite}
\usepackage{algorithm}
\usepackage{multirow}
\usepackage{caption}
\usepackage{algorithmic}
%\usepackage[amsmath,thmmarks]{ntheorem}
\usepackage{listings}
\usepackage{color}
\usepackage{bm}



\newtheorem{thm}{Theorem}
\newtheorem{mydef}{Definition}

\DeclareMathOperator*{\rank}{rank}
\DeclareMathOperator*{\trace}{trace}
\DeclareMathOperator*{\acos}{acos}
\DeclareMathOperator*{\argmax}{argmax}


\renewcommand{\algorithmicrequire}{ \textbf{Input:}}     
\renewcommand{\algorithmicensure}{ \textbf{Output:}}
\renewcommand{\mathbf}{\boldsymbol}
\newcommand{\mb}{\mathbf}
\newcommand{\matlab}[1]{\texttt{#1}}
\newcommand{\setname}[1]{\textsl{#1}}
\newcommand{\Ce}{\mathbb{C}}
\newcommand{\Ee}{\mathbb{E}}
\newcommand{\Ne}{\mathbb{N}}
\newcommand{\Se}{\mathbb{S}}
\newcommand{\norm}[2]{\left\| #1 \right\|_{#2}}

\newenvironment{mfunction}[1]{
	\noindent
	\tabularx{\linewidth}{>{\ttfamily}rX}
	\hline
	\multicolumn{2}{l}{\textbf{Function \matlab{#1}}}\\
	\hline
}{\\\endtabularx}

\newcommand{\parameters}{\multicolumn{2}{l}{\textbf{Parameters}}\\}

\newcommand{\fdescription}[1]{\multicolumn{2}{p{0.96\linewidth}}{
		
		\textbf{Description}
		
		#1}\\\hline}

\newcommand{\retvalues}{\multicolumn{2}{l}{\textbf{Returned values}}\\}
\def\0{\boldsymbol{0}}
\def\b{\boldsymbol{b}}
\def\bmu{\boldsymbol{\mu}}
\def\e{\boldsymbol{e}}
\def\u{\boldsymbol{u}}
\def\x{\boldsymbol{x}}
\def\v{\boldsymbol{v}}
\def\w{\boldsymbol{w}}
\def\N{\boldsymbol{N}}
\def\X{\boldsymbol{X}}
\def\Y{\boldsymbol{Y}}
\def\A{\boldsymbol{A}}
\def\B{\boldsymbol{B}}
\def\y{\boldsymbol{y}}
\def\cX{\mathcal{X}}
\def\transpose{\top} % Vector and Matrix Transpose

%\long\def\answer#1{{\bf ANSWER:} #1}
\long\def\answer#1{}
\newcommand{\myhat}{\widehat}
\long\def\comment#1{}
\newcommand{\eg}{{e.g.,~}}
\newcommand{\ea}{{et al.~}}
\newcommand{\ie}{{i.e.,~}}

\newcommand{\db}{{\boldsymbol{d}}}
\renewcommand{\Re}{{\mathbb{R}}}
\newcommand{\Pe}{{\mathbb{P}}}

\hyphenation{MATLAB}

\usepackage[margin=1in]{geometry}

\begin{document}
	
\title{	Numerical Optimization, 2020 Fall\\Homework $8$}
\date{Due 14:59 (CST), Dec. 10, 2020 \\(NOTE: Homework will not be accepted after this due for any reason.)\\}
\maketitle
Throughout this assignment, we focus on the following trust region subproblem, which reads
\begin{equation}\label{eq: TR_sub}
	\begin{aligned}
		\min_{\bm{d}\in\mathbb{R}^{n}}~&\quad m_{k}(\bm{d}):= f(\bm{x}_{k}) + \nabla f(\bm{x}_{k})^{T}\bm{d}_{k} + \tfrac{1}{2}\bm{d}_{k}^{T}H_{k}\bm{d}_{k}\\
		\textrm{s.t.}~~\ &\quad\Vert\bm{d}\Vert\leq \Delta_{k},
	\end{aligned}
\end{equation}
where $\Delta_{k} > 0$ is the trust-region radius.

Note: Throughout this assignment, the notion of positive definiteness applies exclusively to symmetric matrices. Thus whenever we say that a matrix is positive (semi)definite, we implicitly assume that the matrix is symmetric.
%===============================
\section{Cauchy point calculation}
~\textcolor{red}{[20pts]}~Please write down a closed-form expression of the Cauchy point.~\textcolor{red}{~(Make sure you provided detailed proof; otherwise you won't earn marks.)}

Specifically, first solve the a linear version of~\eqref{eq: TR_sub} to obtain vector $\bm{d}^{s}_{k}$, that is,
\begin{equation}\label{eq:linear_tr}
	\bm{d}^{s}_{k} = \arg\min_{\bm{d}\in\mathbb{R}^{n}}f(\bm{x}_{k}) + \nabla f(\bm{x}_{k})^{T}\bm{d}_{k}\qquad~\textrm{s.t.}\quad\Vert\bm{d}\Vert\leq \Delta_{k}.
\end{equation}

Then, calculate the scalar $\tau_{k}>0$ that minimizes $m_{k}(\tau\bm{d}^{s}_{k})$ subject to the trust region bound, that is
\begin{equation}\label{eq: scalar_cal}
	\tau_{k} = \arg\min_{\tau\geq0}~m_{k}(\tau\bm{d}^{s}_{k})\qquad~\textrm{s.t.}\quad\Vert\tau\bm{d}^{s}_{k}\Vert\leq \Delta_{k}.
\end{equation}

Set $\bm{d}_{k}^{c} = \tau_{k}\bm{d}^{s}_{k}$.\\
\textbf{Solution:}
\begin{itemize}
	\item Step1: $\bm{d}^{s}_{k} $ should lies in the negative direction of gradient $\nabla f(\bm{x}_{k})$ to reach the minimum. Plus, consider the bound of its length, we get:
		$$\bm{d}^{s}_{k} = - \Delta_k  \frac{\nabla f(\bm{x}_{k})}{\Vert \nabla f(\bm{x}_{k}) \Vert}.$$
	\item Step2: To obtain $\tau_k$ explicitly, we consider the cases of $\nabla f(\bm{x}_{k})^T H_k \nabla f(\bm{x}_{k}) \le 0$ and $\nabla f(\bm{x}_{k})^T H_k \nabla f(\bm{x}_{k}) > 0$ separately.
		\begin{itemize}
			\item When $\nabla f(\bm{x}_{k})^T H_k \nabla f(\bm{x}_{k}) \le 0$, 
				$$m_{k}(\tau\bm{d}^{s}_{k}) = f(\bm{x}_{k}) - \tau_k \Delta_k \Vert \nabla f(\bm{x}_{k}) \Vert + \tau_k^2 \frac{\Delta_k^2}{2\Vert \nabla f(\bm{x}_{k}) \Vert^2}  \nabla f(\bm{x}_{k})^T H_k  \nabla f(\bm{x}_{k}),$$
				where the subproblem decreases monotonically with $\tau$ whenever $\nabla f(\bm{x}_{k}) \ne 0$. Thus $\tau_k=1$ in this case.
			\item When $\nabla f(\bm{x}_{k})^T H_k \nabla f(\bm{x}_{k}) \le 0$, the optimal $\tau_k$ should be:
				$$\tau_k = \min\{ \frac{\Delta_k \Vert \nabla f(\bm{x}_{k}) \Vert}{\frac{\Delta_k^2}{\Vert \nabla f(\bm{x}_{k}) \Vert^2} \nabla f(\bm{x}_{k})^T H_k  \nabla f(\bm{x}_{k})}, 1\}.$$
				where we use the fact that $\tau^\star = -\frac{b}{2a}$ if no constraints are given.
		\end{itemize}
	\item In conclusion, we have $$\bm{d}_{k}^{c} = -\tau_{k}\Delta_k  \frac{\nabla f(\bm{x}_{k})}{\Vert \nabla f(\bm{x}_{k}) \Vert},$$where $$\tau_k =\left\{
																																						\begin{array}{ll}
																																						1&{\text{if $\nabla f(\bm{x}_{k})^T H_k \nabla f(\bm{x}_{k}) \le 0$}}\\
																																						 \min\{ \frac{ \Vert \nabla f(\bm{x}_{k}) \Vert^3}{\Delta_k  \nabla f(\bm{x}_{k})^T H_k  \nabla f(\bm{x}_{k})}, 1\}& {\text{otherwise}.}
																																						\end{array} \right. 
																																						$$
\end{itemize}
%===============================
\section{Local convergence for trust region methods}
~\textcolor{red}{[20pts]}~Given a step $\bm{d}_{k}$, consider the ratio (with positive denominator):
\begin{equation}
	\rho_{k} := \frac{f(\bm{x}_{k}) - f(\bm{x}_{k} + \bm{d}_{k}) }{m_{k}(\bm{0}) - m_{k}(\bm{d}_{k})}.
\end{equation}

Show that if $\Delta_{k}\to 0$, then $\rho_{k}\to1$. (This proves that for $\Delta_{k}$ sufficiently small, $m_{k}(\bm{d})$ approximates $f(\bm{x}_{k} + \bm{d}_{k})$ well.)\\
\textbf{Solution:}
\begin{align*}
	\rho_{k} &= \frac{f(\bm{x}_{k}) - f(\bm{x}_{k} + \bm{d}_{k}) }{m_{k}(\bm{0}) - m_{k}(\bm{d}_{k})}\\
	&= \frac{-\nabla f(\bm{x}_{k}) \bm{d}_k -\frac{1}{2} \bm{d}_k^TH_k'\bm{d}_k}{-\nabla f(\bm{x}_{k}) \bm{d}_k - \frac{1}{2}\bm{x}_{k}^TH_k\bm{x}_{k}},
\end{align*}
where $H_k'$ is the Hessian matrix at some point $\bm{x}' \in (\bm{x}_k, \bm{x}_k+\bm{d}_k)$. Since $\bm{d}_k\le \Delta_k\rightarrow 0$, the second order terms  $\bm{d}_k^TH_k'\bm{d}_k$ and $ \bm{d}_k^TH_k\bm{d}_k$ can be omitted comparing to first order term $\nabla f(\bm{x}_{k})$. Thus we get:
	\begin{align*}
		\lim_{\Delta_k\rightarrow0} \rho_{k} & = \lim_{\Delta_k\rightarrow0} \frac{-\nabla f(\bm{x}_{k}) \bm{d}_k -\frac{1}{2} \bm{d}_k^TH_k'\bm{d}_k}{-\nabla f(\bm{x}_{k}) \bm{d}_k - \frac{1}{2}\bm{x}_{k}^TH_k\bm{x}_{k}}\\
		&= \lim_{\Delta_k\rightarrow0} \frac{-\nabla f(\bm{x}_{k}) \bm{d}_k}{-\nabla f(\bm{x}_{k}) \bm{d}_k}\\
		&= 1,
	\end{align*}
	when $\Delta_{k}\to 0$.
%===============================
\section{Exact line search}
~\textcolor{red}{[20pts]}~Consider minimizing the following quadratic function
\begin{equation}\label{eq: quadratic}
	\min_{\bm{x}\in\mathbb{R}^{n}}\quad f(\bm{x}) = \tfrac{1}{2}\bm{x}^{T}Q\bm{x} - \bm{b}^{T}\bm{x},
\end{equation}
where $Q\in\mathbb{R}^{n\times n}$ is positive definite and $\bm{b} \in \mathbb{R}^{n}$.

Let $\bm{d}_{k}$ be a descent direction at the $k$th iterate. Suppose that we search along this direction from $\bm{x}^{k}$ for a new iterate, and the line search are exact. Please find the stepsize $\alpha$. This can be achieved exactly solving the following one-dimensional minimization problem
\begin{equation}
	\min_{\alpha > 0} \quad f(\bm{x}_{k} + \alpha \bm{d}_{k}).
\end{equation}
\textbf{Solution:}\\
First we compute the first and second derivatives with respect to $\alpha$:
\begin{align*}
	f(\bm{x}_{k} + \alpha \bm{d}_{k}) &= \frac{1}{2} (\bm{x}_{k} + \alpha \bm{d}_{k})^TQ(\bm{x}_{k} + \alpha \bm{d}_{k})-\bm{b}^T(\bm{x}_{k} + \alpha \bm{d}_{k}), \\
	\frac{\partial f}{\partial \alpha} &= \bm{x}_k^TQ\bm{d}_k + \alpha \bm{d}_k^TQ \bm{d}_k - \bm{b}^T \bm{d}_k,\\
	\frac{\partial^2 f}{\partial \alpha^2} &= \bm{d}_k^TQ \bm{d}_k \ge 0.
\end{align*}
where the last inequality comes from the positive definiteness of $Q$. Thus we just need to find the $\alpha$ that satisfies the first order necessary condition which gives: 
$$\frac{\partial f}{\partial \alpha}=0 \quad \Rightarrow \quad \alpha^\star = \frac{\bm{b}^T \bm{d}_k-\bm{x}_k^TQ\bm{d}_k}{\bm{d}_k^TQ \bm{d}_k}.$$

%===============================
\section{The conjugate gradient algorithm}
~\textcolor{red}{[20pts]}~Let $A\in \mathbb{R}^{n\times n}$ be a positive definite matrix. Show that if the directions $\bm{d}_{0}, \ldots, \bm{d}_{k}\in\mathbb{R}^{n}$, $k\leq n-1$,  are $A$-conjugate, then they are linearly independent.~\textcolor{red}{(Hint: We say that a set of nonzero vectors $\bm{d}_1,\ldots,\bm{d}_{m}\in\mathbb{R}^{n}$ are $A$-conjugate if $\bm{d}_{i}^{T}A\bm{d}_{i} = 0,\ \  \forall i,j,~~i\neq j$.)}\\
\textbf{Solution:}\\
Assume that $\bm{d}_{0}, \ldots, \bm{d}_{k}\in\mathbb{R}^{n}$ are not linearly independent. Accordingly, we can express $\bm{d}_m = \sum_{i\ne m}\alpha_i \bm{d}_i$. Then we choose $\bm{d}_j$ such that $j\ne m$ and $\alpha_j \ne 0$ (this always works because $\bm{d}_m = \vec{0}$ if not). Then we have:
	\begin{align*}
		\bm{d}_m^TA\bm{d}_j &= (\sum_{i\ne m}\alpha_i\bm{d}_i )^TA\bm{d}_j\\
		&= \alpha_j \bm{d}_j^TA\bm{d}_j\\
		&\ne 0,
	\end{align*}
which violates the fact that $\bm{d}_{0}, \ldots, \bm{d}_{k}$ are $A$-conjugate. The assumption is unreasonable and $\bm{d}_{0}, \ldots, \bm{d}_{k}$ are linearly independent in this case.

%===============================
\section{Trust region subproblems}
Consider the trust region subproblem~\eqref{eq: TR_sub}, and $H_k$ is positive definite. Let $\theta_{k}$ denote the angle between $\bm{d}_{k}$ and $-\nabla f(\bm{x}_{k})$, defined by 
$$\cos\theta_{k} = \frac{-\nabla f(\bm{x}_{k})^{T}\bm{d}_{k}}{\Vert\nabla f(\bm{x}_{k})\Vert\Vert\bm{d}_{k}\Vert}.$$Show that
\begin{itemize}
	\item[(i)] ~\textcolor{red}{[10pts]}~For sufficiently large $\Delta_{k}$, the trust region subproblem~\eqref{eq: TR_sub} will be solved by the Newton step. 
	
	\item[(ii)] ~\textcolor{red}{[10pts]}~When $\Delta_{k}$ approaches $0$, the angle $\theta_{k}\to 0$.
\end{itemize}
\textbf{Solution:}\\
$\bm{d}_k$ is a global solution of the TR subproblem if and only if for Some $\lambda\ge 0$ we have 
\begin{align*}
	(H_k+\lambda I)\bm{d}_k &= -\nabla f(\bm{x}_k),\\
	(H_k + \lambda I) &\ge 0,\\
	\lambda &\ge 0 ,\\
	\lambda (\Delta_k - \Vert \bm{d}_k \Vert) &= 0.
\end{align*}
\begin{itemize}
	\item [(i)] For sufficiently large $\Delta_{k}$, $\bm{x}_k + \bm{d}_k$ will locate within the trust region so that the constraint on the step can be ignored. Accordingly, $\lambda =0 $ and the subproblem is equivalent to finding the Newton direction:
		\begin{align*}
			  (H_k+0I)\bm{d}_k = -\nabla f(\bm{x}_{k}) \quad \Rightarrow \quad  \bm{d}_k = -H_k^{-1}\nabla f(\bm{x}_{k}),
		\end{align*}
		which is basically solved by the Newton step.
	\item [(ii)] When $\Delta_k \rightarrow 0$, then $\bm{d}_k \rightarrow 0$ and $\lambda \rightarrow +\infty$. This conclusion can be seen from meeting the condition $(H_k+\lambda I)\bm{d}_k = -\nabla f(\bm{x}_k)$. In such case, $(H_k+\lambda I)\bm{d}_k\rightarrow \lambda I,\,\lambda \rightarrow +\infty$. Thus we have:
		\begin{align*}
			\bm{d}_k &=- (H_k +\lambda I)^{-1} \nabla f(\bm{x}_k)\\
			&\approx -\frac{1}{\lambda} \nabla f(\bm{x}_k).
		\end{align*}
		Accordingly, the angle $\theta_k =\arccos \frac{-\nabla f(\bm{x}_{k})^{T}\bm{d}_{k}}{\Vert\nabla f(\bm{x}_{k})\Vert\Vert\bm{d}_{k}\Vert}\rightarrow\arccos \frac{\frac{1}{\lambda}\Vert\nabla f(\bm{x}_{k})\Vert^2}{\frac{1}{\lambda} \Vert\nabla f(\bm{x}_{k})\Vert^2}=\arccos 1 = 0$. So $\theta_k\rightarrow0$.
\end{itemize}
\end{document}